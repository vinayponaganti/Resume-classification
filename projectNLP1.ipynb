{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "880c8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from win32com import client\n",
    "import os\n",
    "from docx import Document \n",
    "import PyPDF2\n",
    "import textract\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc5f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning path based on the category\n",
    "files1 = glob(\"D:\\\\dataScience\\\\projects\\\\NLP_RESUME_CLASIFICATION\\\\data_sets\\\\classified_Resumes\\\\Peoplesoft resumes\\\\*\")\n",
    "files2 = glob(\"D:\\\\dataScience\\\\projects\\\\NLP_RESUME_CLASIFICATION\\\\data_sets\\\\classified_Resumes\\\\React JS Developer\\\\*\")\n",
    "files3 = glob(\"D:\\\\dataScience\\\\projects\\\\NLP_RESUME_CLASIFICATION\\\\data_sets\\\\classified_Resumes\\\\SQL Developer Lightning insight\\\\*\")\n",
    "files4 = glob(\"D:\\\\dataScience\\\\projects\\\\NLP_RESUME_CLASIFICATION\\\\data_sets\\\\classified_Resumes\\\\workday resumes\\\\*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4135df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion for reading doc files\n",
    "def docReader(doc_file_name): \n",
    "    ## 1) Initiate an object that interfaces to Word\n",
    "    word = client.Dispatch(\"Word.Application\")\n",
    "    word.Visible = False \n",
    "    \n",
    "    ## 2) Open the Word document to read in\n",
    "    _ = word.Documents.Open(doc_file_name)\n",
    "\n",
    "    ## 3) Extract the paragraphs and close the connections\n",
    "    doc = word.ActiveDocument\n",
    "    paras = doc.Range().text    \n",
    "    doc.Close()\n",
    "    word.Quit()\n",
    "    return paras    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2552c71",
   "metadata": {},
   "outputs": [
    {
     "ename": "com_error",
     "evalue": "(-2147023170, 'The remote procedure call failed.', None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mcom_error\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30208\\2772229535.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfiles1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\x07'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30208\\101379299.py\u001b[0m in \u001b[0;36mdocReader\u001b[1;34m(doc_file_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m## 3) Extract the paragraphs and close the connections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActiveDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mparas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\win32com\\client\\dynamic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    626\u001b[0m             )\n\u001b[0;32m    627\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m                 \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_oleobj_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretEntry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvoke_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom_error\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mERRORS_BAD_CONTEXT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mcom_error\u001b[0m: (-2147023170, 'The remote procedure call failed.', None, None)"
     ]
    }
   ],
   "source": [
    "# Importing the data present in first file (People soft)\n",
    "data1 = []\n",
    "for i in range(len(files1)):\n",
    "    if files1[i].endswith('docx'):\n",
    "        x = docx2txt.process(files1[i])\n",
    "        data1.append(x)\n",
    "    if files1[i].endswith('doc'):\n",
    "        y = docReader(files1[i])\n",
    "        data1.append(y)\n",
    "        [a for a in y.replace('\\x07', '\\r').split('\\r') if a]\n",
    "    if files1[i].endswith('pdf'):\n",
    "        z = PyPDF2.PdfReader(files1[i])\n",
    "        z1 = ''\n",
    "        for j in range(len(z.pages)):\n",
    "            m = z.pages[j].extract_text()\n",
    "            z1 = z1 + m\n",
    "        data1.append(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ad121",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514009e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(data=data1,columns=['data']) # converting to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1585fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['category'] = 'Peoplesoft' # Creating column with column name category and assigning \"peoplesoft\" to every cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Name column and assinging name\n",
    "name1 = []\n",
    "for i in range(len(files1)):\n",
    "    tem = files1[i].split('\\\\')\n",
    "    name1.append(tem[-1])\n",
    "names1 = []\n",
    "for i in range(len(name1)):\n",
    "    d = name1[i].split('.')\n",
    "    names1.append(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10976e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = pd.DataFrame(data = names1,columns=[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.concat([data1,names1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data present in second file (React JS Developer)\n",
    "data2 = []\n",
    "for i in range(len(files2)):\n",
    "    if files2[i].endswith('docx'):\n",
    "        x = docx2txt.process(files2[i])\n",
    "        data2.append(x)\n",
    "    if files2[i].endswith('doc'):\n",
    "        y = docReader(files2[i])\n",
    "        data2.append(y)\n",
    "        [a for a in y.replace('\\x07', '\\r').split('\\r') if a]\n",
    "    if files2[i].endswith('pdf'):\n",
    "        z = PyPDF2.PdfReader(files2[i])\n",
    "        z1 = ''\n",
    "        for j in range(len(z.pages)):\n",
    "            m = z.pages[j].extract_text()\n",
    "            z1 = z1 + m\n",
    "        data2.append(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(data=data2,columns=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['category'] = 'React JS Developer' # Creating column with column name category and assigning \"React JS Developer\" to every cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb711ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Name column and assinging name\n",
    "name2 = []\n",
    "for i in range(len(files2)):\n",
    "    tem = files2[i].split('\\\\')\n",
    "    name2.append(tem[-1])\n",
    "names2 = []\n",
    "for i in range(len(name2)):\n",
    "    d = name2[i].split('.')\n",
    "    names2.append(d[0])\n",
    "names2 = pd.DataFrame(data = names2,columns=[\"Name\"])\n",
    "data2 = pd.concat([data2,names2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc83a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data present in third file (SQL Developer)\n",
    "data3 = []\n",
    "for i in range(len(files3)):\n",
    "    if files3[i].endswith('docx'):\n",
    "        x = docx2txt.process(files3[i])\n",
    "        data3.append(x)\n",
    "    if files3[i].endswith('doc'):\n",
    "        y = docReader(files3[i])\n",
    "        data3.append(y)\n",
    "        [a for a in y.replace('\\x07', '\\r').split('\\r') if a]\n",
    "    if files3[i].endswith('pdf'):\n",
    "        z = PyPDF2.PdfReader(files3[i])\n",
    "        z1 = ''\n",
    "        for j in range(len(z.pages)):\n",
    "            m = z.pages[j].extract_text()\n",
    "            z1 = z1 + m\n",
    "        data3.append(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58206e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.DataFrame(data=data3,columns=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55937b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['category'] = 'SQL Developer' # Creating column with column name category and assigning \"SQL Developer\" to every cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Name column and assinging name\n",
    "name3 = []\n",
    "for i in range(len(files3)):\n",
    "    tem = files3[i].split('\\\\')\n",
    "    name3.append(tem[-1])\n",
    "names3 = []\n",
    "for i in range(len(name3)):\n",
    "    d = name3[i].split('.')\n",
    "    names3.append(d[0])\n",
    "names3 = pd.DataFrame(data = names3,columns=[\"Name\"])\n",
    "data3 = pd.concat([data3,names3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data present in 4th file (workday)\n",
    "data4 = []\n",
    "for i in range(len(files4)):\n",
    "    if files4[i].endswith('docx'):\n",
    "        x = docx2txt.process(files4[i])\n",
    "        data4.append(x)\n",
    "    if files4[i].endswith('doc'):\n",
    "        y = docReader(files4[i])\n",
    "        data4.append(y)\n",
    "        [a for a in y.replace('\\x07', '\\r').split('\\r') if a]\n",
    "    if files4[i].endswith('pdf'):\n",
    "        z = PyPDF2.PdfReader(files4[i])\n",
    "        z1 = ''\n",
    "        for j in range(len(z.pages)):\n",
    "            m = z.pages[j].extract_text()\n",
    "            z1 = z1 + m\n",
    "        data4.append(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ae58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.DataFrame(data=data4,columns=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ce431",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4['category'] = 'workday' # Creating column with column name category and assigning \"workday\" to every cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Name column and assinging name\n",
    "name4 = []\n",
    "for i in range(len(files4)):\n",
    "    tem = files4[i].split('\\\\')\n",
    "    name4.append(tem[-1])\n",
    "names4 = []\n",
    "for i in range(len(name4)):\n",
    "    d = name4[i].split('.')\n",
    "    names4.append(d[0])\n",
    "names4 = pd.DataFrame(data = names4,columns=[\"Name\"])\n",
    "data4 = pd.concat([data4,names4],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3df069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "lst = [data1,data2,data3,data4]\n",
    "for subDF in lst:\n",
    "    df = pd.concat([df, subDF],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc55baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = []\n",
    "for i in range(len(df.data)):\n",
    "    ts = test(\" \".join(df.data[i].split('\\n'))) # we have splitted our data with '\\n' and rejoined with space. \n",
    "    tt = []\n",
    "    for ent in ts.ents:\n",
    "      if ent.label_.upper() == 'ORG':\n",
    "        tt.append(ent.text)\n",
    "    skills.append(tt) # appending all skills to the list skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['skills']=0  # creating new columns skills and assiging 0 to every column\n",
    "for i in range(len(df.skills)):\n",
    "    df.skills[i] = skills[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd498d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('data', axis=1, inplace=True) # it will drop the column data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ced7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.skills)):\n",
    "    lower_words=[Text.lower() for Text in df.skills[i]]\n",
    "    df.skills[i] = lower_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d488ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.skills)):\n",
    "    ab =[]\n",
    "    for j in range(len(df.skills[i])):\n",
    "        jk = re.split(r'[,(\\n\\t:]', df.skills[i][j]) # splitting the objects using ,,(,\\n,\\t,:\n",
    "        ab = jk + ab\n",
    "    df.skills[i] = ab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb324636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d73aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.skills)):\n",
    "    lm = set(df.skills[i])\n",
    "    df.skills[i] = list(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.skills)):\n",
    "    df.skills[i] = \" \".join(df.skills[i]) # converting list into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b432e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "for i in range(len(df.skills)):\n",
    "    df.skills[i] = word_tokenize(df.skills[i]) # tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # importing stop words\n",
    "\n",
    "my_stop_words = stopwords.words('english')\n",
    "my_stop_words\n",
    "\n",
    "my_stop_words.append(' ')\n",
    "my_stop_words.append('&') # adding reqiued stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "for i in range(len(df.skills)):    \n",
    "    df.skills[i] = [word for word in df.skills[i] if not word in my_stop_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the words into single document (removing the tokenization)\n",
    "for i in range(len(df.skills)):\n",
    "    df.skills[i] =  ' '.join(df.skills[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45556849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(df.skills)):\n",
    "    lemmas = []\n",
    "    for token in df.skills[i].split():\n",
    "        lemmas.append(Lemmatizer.lemmatize(token))\n",
    "    df.skills[i] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db58b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining the words into single document (removing the tokenization)\n",
    "for i in range(len(df.skills)):\n",
    "    df.skills[i] =  ' '.join(df.skills[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['skills']\n",
    "y = df['category']\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True)\n",
    "word_vectorizer.fit(x)\n",
    "x = word_vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=30, test_size=0.20, shuffle = True, stratify=y)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1dcb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "Knn = KNeighborsClassifier(n_neighbors=5,p=2)\n",
    "Knn.fit(x_train, y_train)\n",
    "y_pred_train = Knn.predict(x_train)\n",
    "y_pred_test = Knn.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,roc_auc_score,confusion_matrix\n",
    "\n",
    "print(\"Train Accuracy :\",accuracy_score(y_train, y_pred_train))\n",
    "print(\"Test accuracy : \",accuracy_score(y_test, y_pred_test))\n",
    "print(\"Train recall score is \",recall_score(y_train, y_pred_train,average = 'macro'))\n",
    "print(\"Test recall score is \",recall_score(y_test, y_pred_test,average = 'macro'))\n",
    "print(\"Train precision score :\",precision_score(y_train, y_pred_train,average = 'macro'))\n",
    "print(\"Test precision score :\",precision_score(y_test, y_pred_test,average = 'macro'))\n",
    "print(\"Train f1 score is \",f1_score(y_train, y_pred_train,average = 'macro'))\n",
    "print(\"Test f1 score is \",f1_score(y_test, y_pred_test,average = 'macro'))\n",
    "print(\"Train confusion matrix : \\n\",confusion_matrix(y_train, y_pred_train))\n",
    "print(\"Test confusion matrix : \\n\",confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"modelNLP1.pkl\",'wb')\n",
    "pickle.dump(Knn,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e15149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
